import streamlit as st
import os
from datetime import datetime


# === å¿…é¡»åœ¨æœ€å¼€å¤´åˆå§‹åŒ– session state ===
if 'search_history' not in st.session_state:
    st.session_state.search_history = []
if 'current_query' not in st.session_state:
    st.session_state.current_query = ""

from langchain_chroma import Chroma
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableParallel



# è®¾ç½®é¡µé¢åŸºæœ¬ä¿¡æ¯
st.set_page_config(page_title="ã€Šæ„å»ºä¹‹æ³•ã€‹æ™ºèƒ½åŠ©æ•™", layout="wide")
st.title("ã€Šæ„å»ºä¹‹æ³•ã€‹æ™ºèƒ½åŠ©æ•™ (æœ¬åœ°æµ‹è¯•ç‰ˆ)")

@st.cache_resource
def init_system():
    print("\n[DEBUG] --- ç³»ç»Ÿåˆå§‹åŒ–å¼€å§‹ ---")
    persist_dir = "./chroma_db"
    knowledge_dir = "./knowledge_base"  # ç§»åˆ°å‡½æ•°å¼€å¤´

    # 1. åˆå§‹åŒ– Embedding æ¨¡å‹
    print("[DEBUG] æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
    try:
        embeddings = HuggingFaceEmbeddings(model_name="GanymedeNil/text2vec-large-chinese")
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç¼“å­˜: {e}")
        return None

    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºï¼ˆæ–°å¢çš„é€»è¾‘ï¼‰
    need_rebuild = needs_rebuild(persist_dir, knowledge_dir)
    
    # å¦‚æœä¸éœ€è¦é‡å»ºä¸”æ•°æ®åº“å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
    if not need_rebuild:
        db_file_path = os.path.join(persist_dir, "chroma.sqlite3")
        if os.path.exists(db_file_path):
            print(f"[DEBUG] âœ… å‘ç°æœ¬åœ°æ•°æ®åº“ ({db_file_path})ï¼Œæ­£åœ¨ç›´æ¥åŠ è½½...")
            try:
                vectordb = Chroma(
                    persist_directory=persist_dir,
                    embedding_function=embeddings
                )
                print("[DEBUG] æœ¬åœ°æ•°æ®åº“åŠ è½½æˆåŠŸï¼")
                return vectordb
            except Exception as e:
                print(f"[DEBUG] âš ï¸ æœ¬åœ°æ•°æ®åº“åŠ è½½å‡ºé”™ï¼Œå°†å°è¯•é‡æ–°æ„å»º: {e}")
                need_rebuild = True

    # 3. éœ€è¦é‡å»ºæˆ–æ•°æ®åº“ä¸å­˜åœ¨
    print("[DEBUG] âš ï¸ å¼€å§‹æ„å»º/æ›´æ–°å‘é‡æ•°æ®åº“ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")

    if not os.path.exists(knowledge_dir):
        st.error(f"çŸ¥è¯†åº“æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{knowledge_dir}")
        return None

    # å®šä¹‰æ”¯æŒ GBK çš„åŠ è½½å™¨
    class CustomTextLoader(TextLoader):
        def __init__(self, file_path: str):
            super().__init__(file_path, encoding="gbk")
        def lazy_load(self):
            try:
                yield from super().lazy_load()
            except Exception as e:
                # å¦‚æœ GBK å¤±è´¥ï¼Œå°è¯• utf-8 å®¹é”™
                try:
                    self.encoding = "utf-8"
                    yield from super().lazy_load()
                except Exception as e2:
                    st.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {self.file_path}: {e}")
                    return

    # åŠ è½½æ–‡ä»¶
    print("[DEBUG] å¼€å§‹æ‰«æå¹¶åŠ è½½æ–‡æ¡£...")
    loader = DirectoryLoader(
        knowledge_dir,
        glob="**/*.txt",
        loader_cls=CustomTextLoader,
        show_progress=True
    )

    documents = loader.load()
    if not documents:
        st.error("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ knowledge_base æ–‡ä»¶å¤¹")
        return None
    print(f"[DEBUG] æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

    # åˆ‡åˆ†æ–‡æœ¬
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    print(f"[DEBUG] æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(texts)} ä¸ªå—")

    # æ„å»ºå¹¶ä¿å­˜æ•°æ®åº“
    print("[DEBUG] æ­£åœ¨è®¡ç®—å‘é‡å¹¶å†™å…¥æ•°æ®åº“ (Chroma)...")
    vectordb = Chroma.from_documents(
        documents=texts,
        embedding=embeddings,
        persist_directory=persist_dir
    )
    print("[DEBUG] âœ… æ•°æ®åº“æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜ï¼")

    return vectordb

def needs_rebuild(persist_dir, knowledge_dir):
    """
    æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“
    è¿”å› True å¦‚æœéœ€è¦é‡å»ºï¼ŒFalse å¦‚æœå¯ä»¥ä½¿ç”¨ç°æœ‰æ•°æ®åº“
    """
    # æ£€æŸ¥æ•°æ®åº“ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(persist_dir):
        print("[DEBUG] å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨ï¼Œéœ€è¦æ„å»º")
        return True
    
    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    db_file = os.path.join(persist_dir, "chroma.sqlite3")
    if not os.path.exists(db_file):
        print("[DEBUG] å‘é‡æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ„å»º")
        return True
    
    # æ£€æŸ¥çŸ¥è¯†åº“ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(knowledge_dir):
        print("[DEBUG] çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨")
        return False
    
    # è·å–æ•°æ®åº“çš„æœ€åä¿®æ”¹æ—¶é—´
    try:
        db_mtime = os.path.getmtime(db_file)
    except OSError:
        print("[DEBUG] æ— æ³•è·å–æ•°æ®åº“æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼Œéœ€è¦é‡å»º")
        return True
    
    # éå†çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰txtæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶æ¯”æ•°æ®åº“æ›´æ–°
    for root, dirs, files in os.walk(knowledge_dir):
        for file in files:
            if file.endswith('.txt'):
                file_path = os.path.join(root, file)
                try:
                    file_mtime = os.path.getmtime(file_path)
                    if file_mtime > db_mtime:
                        print(f"[DEBUG] æ£€æµ‹åˆ°æ›´æ–°çš„æ–‡ä»¶: {file}ï¼Œéœ€è¦é‡å»ºæ•°æ®åº“")
                        return True
                except OSError:
                    # å¦‚æœæ— æ³•è·å–æŸä¸ªæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´ï¼Œç»§ç»­æ£€æŸ¥å…¶ä»–æ–‡ä»¶
                    continue
    
    print("[DEBUG] çŸ¥è¯†åº“æ— æ›´æ–°ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®åº“")
    return False

# åˆå§‹åŒ–ç³»ç»Ÿ
vectordb = init_system()

def create_retrieval_chain(retriever):
    # ä¿®å¤åçš„ LCEL é“¾ï¼š
    # 1. å¹¶è¡Œæ‰§è¡Œï¼šæ£€ç´¢æ–‡æ¡£(source_documents) å’Œ é€ä¼ é—®é¢˜(current_query)
    step1 = RunnableParallel(
        source_documents=retriever,
        current_query=RunnablePassthrough()
    )

    # 2. å¦‚æœæœªæ¥æ¥å…¥ LLMï¼Œå¯ä»¥åœ¨è¿™é‡Œç”¨ .assign() æ·»åŠ  context å’Œ prompt
    # ç›®å‰ MVP é˜¶æ®µï¼Œæˆ‘ä»¬åªéœ€è¦ step1 çš„ç»“æœæ¥å±•ç¤ºæ£€ç´¢åˆ°çš„å†…å®¹
    return step1

if vectordb:
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("è®¾ç½®")
        k_val = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡ (K)", min_value=3, max_value=10, value=5)
        # æœç´¢å†å²åŒºåŸŸ
        st.header("ğŸ“š æœç´¢å†å²")
    
        if st.session_state.search_history:
            # æ˜¾ç¤ºæœ€è¿‘çš„æœç´¢è®°å½•ï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
            for i, history_item in enumerate(reversed(st.session_state.search_history[-10:])):  # åªæ˜¾ç¤ºæœ€è¿‘10æ¡
                query, timestamp = history_item
                time_str = timestamp.strftime("%H:%M")
                
                # ç‚¹å‡»å†å²è®°å½•å¯ä»¥é‡æ–°æœç´¢
                if st.button(f"{i+1}. {query}", key=f"history_{i}"):
                    st.session_state.current_query = query
                    st.rerun()
        else:
            st.caption("æš‚æ— æœç´¢å†å²")

    retriever = vectordb.as_retriever(search_kwargs={"k": k_val})
    retrieval_chain = create_retrieval_chain(retriever)

    # ä¸»ç•Œé¢è¾“å…¥
    current_query = st.text_input("è¯·è¾“å…¥å…³äºã€Šæ„å»ºä¹‹æ³•ã€‹çš„é—®é¢˜ï¼š", placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯ç»“å¯¹ç¼–ç¨‹ï¼Ÿ")

    if current_query:
        # æ·»åŠ åˆ°æœç´¢å†å²ï¼ˆé¿å…é‡å¤æ·»åŠ ç›¸åŒçš„æŸ¥è¯¢ï¼‰
        if not st.session_state.search_history or st.session_state.search_history[-1][0] != current_query:
            st.session_state.search_history.append((current_query, datetime.now()))
        
        with st.spinner('æ­£åœ¨ä¹¦ä¸­ä¸ºæ‚¨å¯»æ‰¾ç­”æ¡ˆ...'):
            result = retrieval_chain.invoke(current_query)

    if current_query:
        with st.spinner('æ­£åœ¨ä¹¦ä¸­ä¸ºæ‚¨å¯»æ‰¾ç­”æ¡ˆ...'):
            # æ‰§è¡Œæ£€ç´¢
            result = retrieval_chain.invoke(current_query)

        # ç»“æœå±•ç¤ºåŒº
        st.subheader("ğŸ“– ä¹¦ä¸­ç›¸å…³åŸæ–‡ç‰‡æ®µï¼š")

        # ç»“æœæ ¡éªŒ
        if not result.get('source_documents'):
            st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
        else:
            for i, doc in enumerate(result['source_documents']):
                source_name = os.path.basename(doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶'))
                with st.expander(f"å‚è€ƒç‰‡æ®µ {i+1} (æ¥æº: {source_name})", expanded=True):
                    st.markdown(f"**åŸæ–‡å†…å®¹ï¼š**\n\n{doc.page_content}")
                    st.caption(f"å…ƒæ•°æ®: {doc.metadata}")
else:
    st.info("ç³»ç»Ÿæ­£åœ¨åˆå§‹åŒ–ï¼Œè¯·æŸ¥çœ‹ç»ˆç«¯è¾“å‡º...")

