import streamlit as st
import os
import hashlib
import json
from datetime import datetime
import shutil

# === 1. åˆå§‹åŒ– Session State (å¿…é¡»åœ¨æœ€å‰) ===
if 'search_history' not in st.session_state:
    st.session_state.search_history = []
if 'current_query' not in st.session_state:
    st.session_state.current_query = ""
if 'input_key' not in st.session_state:
    st.session_state.input_key = 0

from langchain_chroma import Chroma
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_core.runnables import RunnablePassthrough, RunnableParallel

st.set_page_config(page_title="ã€Šæ„å»ºä¹‹æ³•ã€‹æ™ºèƒ½åŠ©æ•™", layout="wide")
st.title("ã€Šæ„å»ºä¹‹æ³•ã€‹æ™ºèƒ½åŠ©æ•™ (å¢é‡æ›´æ–°ç‰ˆ)")

# === å·¥å…·å‡½æ•° ===

def get_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
    try:
        with open(file_path, 'rb') as f:
            file_content = f.read()
            file_stat = os.stat(file_path)
            combined_data = file_content + str(file_stat.st_size).encode()
            return hashlib.md5(combined_data).hexdigest()
    except Exception as e:
        print(f"[DEBUG] æ— æ³•è®¡ç®—æ–‡ä»¶å“ˆå¸Œ {file_path}: {e}")
        return None

def load_file_metadata(persist_dir):
    """åŠ è½½æ–‡ä»¶å…ƒæ•°æ®"""
    metadata_file = os.path.join(persist_dir, "file_metadata.json")
    if os.path.exists(metadata_file):
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"[DEBUG] ä» {metadata_file} åŠ è½½äº† {len(data)} ä¸ªæ–‡ä»¶çš„å…ƒæ•°æ®")
                return data
        except Exception as e:
            print(f"[DEBUG] åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
            return {}
    else:
        print(f"[DEBUG] å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
    return {}

def save_file_metadata(persist_dir, metadata):
    """ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(persist_dir, exist_ok=True)
    
    metadata_file = os.path.join(persist_dir, "file_metadata.json")
    try:
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        print(f"[DEBUG] âœ… å·²ä¿å­˜ {len(metadata)} ä¸ªæ–‡ä»¶çš„å…ƒæ•°æ®åˆ° {metadata_file}")
    except Exception as e:
        print(f"[DEBUG] âŒ æ— æ³•ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®: {e}")

def get_changed_files(knowledge_dir, existing_metadata):
    """è·å–éœ€è¦æ›´æ–°çš„æ–‡ä»¶åˆ—è¡¨"""
    changed_files = []
    new_files = []
    
    if not os.path.exists(knowledge_dir):
        print(f"[DEBUG] çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {knowledge_dir}")
        return [], [], []
    
    # æ‰«æå½“å‰ç£ç›˜ä¸Šçš„æ–‡ä»¶
    current_files_set = set()
    for root, dirs, files in os.walk(knowledge_dir):
        for file in files:
            if file.endswith('.txt'):
                file_path = os.path.join(root, file)
                current_files_set.add(file_path)
                
                current_hash = get_file_hash(file_path)
                if not current_hash: 
                    continue
                
                if file_path in existing_metadata:
                    if existing_metadata[file_path]['hash'] != current_hash:
                        changed_files.append(file_path)  # å†…å®¹å˜äº†
                        print(f"[DEBUG] æ£€æµ‹åˆ°æ–‡ä»¶ä¿®æ”¹: {os.path.basename(file_path)}")
                else:
                    new_files.append(file_path)  # è¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„æ–‡ä»¶
                    print(f"[DEBUG] æ£€æµ‹åˆ°æ–°æ–‡ä»¶: {os.path.basename(file_path)}")
    
    # æ£€æŸ¥æœ‰å“ªäº›æ–‡ä»¶åœ¨å…ƒæ•°æ®é‡Œæœ‰ï¼Œä½†ç£ç›˜ä¸Šåˆ äº†
    deleted_files = [f for f in existing_metadata if f not in current_files_set]
    for deleted_file in deleted_files:
        print(f"[DEBUG] æ£€æµ‹åˆ°æ–‡ä»¶åˆ é™¤: {os.path.basename(deleted_file)}")
    
    return changed_files, new_files, deleted_files

def update_vector_database(vectordb, knowledge_dir, existing_metadata):
    """æ‰§è¡Œå¢é‡æ›´æ–° - ç®€å•è¿›åº¦æ˜¾ç¤º"""
    print("[DEBUG] ğŸ”„ æ£€æŸ¥å¢é‡æ›´æ–°...")
    
    changed_files, new_files, deleted_files = get_changed_files(knowledge_dir, existing_metadata)
    
    if not changed_files and not new_files and not deleted_files:
        print("[DEBUG] âœ… æ‰€æœ‰æ–‡ä»¶å·²æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°")
        return existing_metadata
    
    print(f"[DEBUG] å˜æ›´ç»Ÿè®¡: æ–°å¢ {len(new_files)}, ä¿®æ”¹ {len(changed_files)}, åˆ é™¤ {len(deleted_files)}")

    # 1. å¤„ç†åˆ é™¤çš„æ–‡ä»¶
    for file_path in deleted_files:
        try:
            vectordb._collection.delete(where={"source": file_path})
            if file_path in existing_metadata:
                del existing_metadata[file_path]
            print(f"[DEBUG] âœ… å·²åˆ é™¤æ— æ•ˆç´¢å¼•: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"[DEBUG] âŒ åˆ é™¤å¤±è´¥: {e}")

    # 2. å¤„ç†ä¿®æ”¹å’Œæ–°å¢çš„æ–‡ä»¶
    files_to_process = changed_files + new_files
    
    # æ¸…ç†ä¿®æ”¹æ–‡ä»¶çš„æ—§å‘é‡
    for file_path in changed_files:
        try:
            vectordb._collection.delete(where={"source": file_path})
            print(f"[DEBUG] âœ… å·²æ¸…ç†æ—§å‘é‡: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"[DEBUG] âŒ æ¸…ç†æ—§å‘é‡å¤±è´¥: {e}")

    # å®šä¹‰åŠ è½½å™¨
    class CustomTextLoader(TextLoader):
        def __init__(self, file_path: str):
            super().__init__(file_path, encoding="gbk")
        def lazy_load(self):
            try:
                yield from super().lazy_load()
            except Exception as e:
                try:
                    self.encoding = "utf-8"
                    yield from super().lazy_load()
                except:
                    print(f"[DEBUG] âŒ æ— æ³•è¯»å–æ–‡ä»¶ {self.file_path}")
                    return

    if files_to_process:
        print(f"[DEBUG] æ­£åœ¨å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶...")
        documents = []
        for file_path in files_to_process:
            try:
                loader = CustomTextLoader(file_path)
                file_docs = loader.load()
                documents.extend(file_docs)
                print(f"[DEBUG] âœ… æˆåŠŸåŠ è½½: {os.path.basename(file_path)}")
            except Exception as e:
                print(f"[DEBUG] âŒ è·³è¿‡æ–‡ä»¶ {file_path}: {e}")

        if documents:
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            texts = text_splitter.split_documents(documents)
            print(f"[DEBUG] æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(texts)} ä¸ªå—")
            
            if texts:
                # åˆ†æ‰¹å†™å…¥ï¼Œæ¯10ä¸ªå—è¾“å‡ºä¸€æ¬¡è¿›åº¦
                batch_size = 10
                total_batches = (len(texts) + batch_size - 1) // batch_size
                
                print(f"[DEBUG] å¼€å§‹åˆ†æ‰¹å†™å…¥ï¼Œå…± {total_batches} æ‰¹...")
                
                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i + batch_size]
                    vectordb.add_documents(batch)
                    
                    current_batch = i // batch_size + 1
                    processed = min(i + batch_size, len(texts))
                    print(f"[DEBUG] âœ… å·²å®Œæˆç¬¬ {current_batch}/{total_batches} æ‰¹ï¼Œå·²å¤„ç† {processed}/{len(texts)} ä¸ªå—")
                
                print(f"[DEBUG] âœ… æ‰€æœ‰ {len(texts)} ä¸ªæ–‡æœ¬å—å·²æˆåŠŸæ·»åŠ åˆ°æ•°æ®åº“")
                
                # æ›´æ–°å…ƒæ•°æ®
                for file_path in files_to_process:
                    file_hash = get_file_hash(file_path)
                    if file_hash:
                        existing_metadata[file_path] = {
                            'hash': file_hash,
                            'last_updated': datetime.now().isoformat(),
                            'chunk_count': len([t for t in texts if t.metadata.get('source') == file_path])
                        }
    else:
        print("[DEBUG] æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶")

    return existing_metadata


@st.cache_resource
def init_system():
    print("\n[DEBUG] --- ç³»ç»Ÿåˆå§‹åŒ– ---")
    persist_dir = "./chroma_db"
    knowledge_dir = "./knowledge_base"
    
    # ç¡®ä¿çŸ¥è¯†åº“ç›®å½•å­˜åœ¨
    if not os.path.exists(knowledge_dir):
        st.error(f"âŒ çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {knowledge_dir}")
        return None
    
    # 1. åŠ è½½ Embedding
    try:
        print("[DEBUG] æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
        embeddings = HuggingFaceEmbeddings(model_name="GanymedeNil/text2vec-large-chinese")
    except Exception as e:
        st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

    # 2. åˆå§‹åŒ–/åŠ è½½ Chroma
    print("[DEBUG] æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
    try:
        vectordb = Chroma(
            persist_directory=persist_dir,
            embedding_function=embeddings
        )
    except Exception as e:
        st.error(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        return None
    
    # 3. åŠ è½½å…ƒæ•°æ®å¹¶æ‰§è¡Œå¢é‡æ›´æ–°
    file_metadata = load_file_metadata(persist_dir)
    updated_metadata = update_vector_database(vectordb, knowledge_dir, file_metadata)
    save_file_metadata(persist_dir, updated_metadata)
    
    print("[DEBUG] âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    return vectordb

# === ç³»ç»Ÿåˆå§‹åŒ– ===
vectordb = init_system()

def create_retrieval_chain(retriever):
    return RunnableParallel(
        source_documents=retriever,
        question=RunnablePassthrough()
    )

# === ç•Œé¢é€»è¾‘ ===
if vectordb:
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("è®¾ç½®")
        k_val = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡ (K)", 3, 10, 3)
        
        st.divider()
        st.header("ğŸ“š æœç´¢å†å²")
        
        # å†å²è®°å½•ç‚¹å‡»å¤„ç†
        if st.session_state.search_history:
            for i, (hist_query, timestamp) in enumerate(reversed(st.session_state.search_history[-10:])):
                if st.button(f"{hist_query}", key=f"hist_{i}"):
                    st.session_state.current_query = hist_query
                    st.session_state.input_key += 1
                    st.rerun()
        else:
            st.caption("æš‚æ— å†å²")

        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®"):
            st.cache_resource.clear()
            if os.path.exists("./chroma_db"):
                shutil.rmtree("./chroma_db")
            st.success("å·²é‡ç½®ï¼Œè¯·åˆ·æ–°é¡µé¢")
            st.rerun()

    # ä¸»ç•Œé¢
    retriever = vectordb.as_retriever(search_kwargs={"k": k_val})
    chain = create_retrieval_chain(retriever)

    # æœç´¢æ¡†
    query = st.text_input(
        "è¯·è¾“å…¥é—®é¢˜ï¼š", 
        value=st.session_state.current_query,
        key=f"search_input_{st.session_state.input_key}" 
    )

    # æ‰§è¡Œæœç´¢é€»è¾‘
    if query:
        # å¦‚æœæ˜¯æ–°è¾“å…¥çš„å†…å®¹ï¼Œæ›´æ–° Session å¹¶ä¿å­˜å†å²
        if query != st.session_state.current_query:
            st.session_state.current_query = query
        
        # æ·»åŠ å†å²è®°å½• (å»é‡)
        if not st.session_state.search_history or st.session_state.search_history[-1][0] != query:
            st.session_state.search_history.append((query, datetime.now()))

        with st.spinner('ğŸ” æ­£åœ¨æ£€ç´¢...'):
            result = chain.invoke(query)
            
            # ç»“æœå±•ç¤º
            if result.get('source_documents'):
                st.subheader(f"æ‰¾åˆ° {len(result['source_documents'])} ä¸ªç›¸å…³ç‰‡æ®µï¼š")
                for i, doc in enumerate(result['source_documents']):
                    src = os.path.basename(doc.metadata.get('source', 'æœªçŸ¥'))
                    with st.expander(f"å‚è€ƒ {i+1}: {src}", expanded=(i == 0)):  # åªå±•å¼€ç¬¬ä¸€ä¸ª
                        st.markdown(doc.page_content)
                        st.caption(f"æ¥æº: {src}")
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚")

else:
    st.info("ç³»ç»Ÿåˆå§‹åŒ–ä¸­...")